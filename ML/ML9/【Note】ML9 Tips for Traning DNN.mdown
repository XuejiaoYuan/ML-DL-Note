# 【Note】ML9 Tips for Traning DNN

> 根据台大李宏毅教授的机器学习2017课程做笔记；
> 
> [李宏毅教授主页](http://speech.ee.ntu.edu.tw/~tlkagk/index.html)
> 
> 2018.07.04

在训练DNN时可能存在训练数据结果糟糕和测试数据结果糟糕两种情况，针对这两种情况进行处理的方式不同。
需要明确，在training set上模型结果不够理想并不是overfitting的原因，overfitting表现为训练集结果理想，
而测试集结果糟糕。

<img src="1.png" width="400">

## 优化DNN
### 优化训练数据结果
若模型在训练数据上不能获得较理想的结果，则需要先对模型进行调整，包括两种方法：
- New activation function
- Adaptive learning rate

#### New activation function
**1. Vanishing Gradient Problem**

当网络层数过深时，可能会出现vanishing gradient的现象，在接近输入层的若干层梯度较小，而接近输出的若干层梯度较大，
若每层的学习率相同，则靠近输入层的若干层学习速度较慢，靠近输出的若干层学习速度较快。则会出现后几层参数收敛时，
前几层参数仍为随机，且由于后几层的输入依赖于前几层的输出，会导致学习的结果较差。

上述情况源于sigmoid函数的使用。若要粗略估计损失函数与参数之间的微分关系，可以通过对参数设置一个微小的变化量，
观察输出损失函数的变化量来判断两者之间的关系。由于sigmoid函数会将输入衰减至[0,1]范围内，若网络很深，则每通过
一次sigmoid函数，变化量将变得越来越小。因此为了不出现梯度消失的问题，可以使用ReLU替换sigmoid函数。

**2. ReLU**

Rectified Linear Unit(ReLU)

<img src="2.png" width="400">

原因：
- 计算速度快;
- 生物学理由；
- 可以避免梯度消失问题；

使用ReLU后，整个网络会变成稀疏线性的，不会出现梯度消失的问题；与线性函数不同的是，在这里ReLU可以认为是无数的分段函数，
只有在很小的邻域内属于线性，整体为非线性的。

**3. Maxout**
<img src="3.png" width="400">

ReLU是Maxout的一种特殊情况。Maxout即是比较输入加权计算结果的大小，并输出最大值的函数，由上图可知，这种形式计算结果，
相当于自主学习activation函数，对于这种方式：
- 在maxout网络中的激活函数是分段线性函数；
- 分段数量取决于一个max组内有多少变量；

#### Adaptive Learning Rate

**RMSProp**

<img src="4.png" width="400">


**Momentum**

<img src="5.png" width="400">

借鉴物体滚落时的惯性概念，当参数移动到梯度为0的位置，由于存在惯性，因此参数仍然会保持原方向变化。但这个方式不能保证最后得到的
结果能够脱离局部最优的情况。

**1.Vanilla Gradient Descent**

<img src="6.png" width="400">

普通的梯度下降方式。

**2. Momentum**

<img src="7.png" width="400">

Momentun是综合过去所有移动的结果。

**3. Adam**

综合Momentum和RMSProp的结果就是Adam方法。

### 优化测试数据结果
训练数据理想而测试数据不理想是overfitting的情况，有三种方法可以解决：
- Early Stopping；
- Regularization；
- Dropout；

#### Early Stopping
在训练模型时，测试集和训练集的错误率会越来越小，但是测试集与训练集的错误率变化并不一致，即训练集错误率进一步降低时，测试集的错误
率可能增大，因此我们希望能够将模型的训练停止在测试集错误率较小的位置上。但是实际上，我们并不能知道测试集的错误率，因此需要使用验证集，
即在每次训练一个epoch后，我们使用验证集计算一次模型的错误率，若验证集的错误率不再下降，则可停止训练。

#### Regularization

<img src="8.png" width="400">

正则化即是将模型中无用的连接删除。在使用正则化之前，损失函数只考虑cross entropy，但在使用正则化之后引入了正则项，希望参数值越小越好。
在整理公式后可以看到，由于(1-yita*lambda)这一项小于1，若每次更新都乘以该值，则w将减小至0，但由于后面存在偏微分的约束，对于对损失有
影响的w参数而言，将不会变为0，而是在前后两项中取得平衡。

#### Dropout
<img src="9.png" width="400">

即在每次训练参数时，以一定的概率将神经元剔除，则模型会更加的稀疏。

<img src="10.png" width="400">

需要注意在测试时的参数与训练时的参数是不相同的，由于在训练参数时会将部分神经元丢弃，而测试时使用所有的神经元进行计算，因此若不调整
测试是的参数，得到的结果会是训练结果的1/p倍。

**Ensemble**

Dropout是一种Ensemble，Ensemble即是使用训练数据训练多个网络，并在测试时将测试数据输入每一个网络中，将得到的结果求均值作为最后的输出结果。
而Dropout在对每个参数乘以(1-p)后的测试得到的结果与Ensemble的效果相同，尽管证明两者效果的方式仅在激活函数为线性时成立。

<img src="11.png" width="400">
